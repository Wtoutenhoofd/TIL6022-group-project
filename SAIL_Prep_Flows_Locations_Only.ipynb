{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAIL 2025 – Prep Notebook (Flows + Sensor Locations → Dashboard Files)\n",
        "\n",
        "Deze notebook:\n",
        "1. Laadt je **crowd flow** dataset (3-minuten resolutie) en **sensor-locaties**.\n",
        "2. Maakt drie outputbestanden die direct bruikbaar zijn in je dashboard/notebooks:\n",
        "   - `/mnt/data/crowd_timeseries.csv` – long-form timeseries met `timestamp, zone_id, density` (density = flow per 3 min als proxy).\n",
        "   - `/mnt/data/zones.geojson` – simpele polygonen rond sensoren voor choropleth-kaart.\n",
        "   - `/mnt/data/crowd_timeseries_features.csv` – zelfde als `crowd_timeseries.csv` maar inclusief lege kolommen voor AIS/KNMI/WBGT (zodat je later eenvoudig kunt joinen).\n",
        "3. **Nodig om te draaien:** alleen jouw twee bestaande CSV-bestanden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Bestanden (alleen deze twee zijn nu nodig)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# <<< VERVANG ALLEEN ALS JE EIGEN PAD WILT GEBRUIKEN >>>\n",
        "FLOW_CSV = Path(\"/mnt/data/SAIL2025_LVMA_data_3min_20August-25August2025_flow.csv\")\n",
        "LOC_CSV  = Path(\"/mnt/data/sensor-location.xlsx - Sheet1.csv\")\n",
        "\n",
        "# Output-bestanden (laat zo staan)\n",
        "OUT_TIMESERIES = Path(\"/mnt/data/crowd_timeseries.csv\")\n",
        "OUT_GEOJSON    = Path(\"/mnt/data/zones.geojson\")\n",
        "OUT_FEATURES   = Path(\"/mnt/data/crowd_timeseries_features.csv\")\n",
        "\n",
        "# Timeseries is 3-min resolutie\n",
        "STEP_MINUTES = 3\n",
        "print(FLOW_CSV.exists(), LOC_CSV.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Preview data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Lees een snelle peek om kolommen te zien\n",
        "df_preview = pd.read_csv(FLOW_CSV, nrows=3)\n",
        "loc_preview = pd.read_csv(LOC_CSV, nrows=3)\n",
        "display(df_preview.head(3))\n",
        "display(df_preview.columns.tolist())\n",
        "display(loc_preview.head(3))\n",
        "display(loc_preview.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Flow → long-form timeseries\n",
        "\n",
        "- We detecteren alle sensorkolommen (alles behalve `timestamp`, helper-kolommen).\n",
        "- Sensoren met richting-suffix (bijv. `_0`, `_180`) worden **samengevoegd per basenaam** (som over richtingen).\n",
        "- We vormen `density = flow_3min` als eenvoudige proxy (later kun je dit kalibreren)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4.a Lees volledige flow CSV\n",
        "df = pd.read_csv(FLOW_CSV)\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# Helper/metadata kolommen die we NIET willen optellen\n",
        "helper_cols = {'hour','minute','day','month','weekday','is_weekend','timestamp'}\n",
        "sensor_cols = [c for c in df.columns if c not in helper_cols]\n",
        "\n",
        "# Functie om basenaam (zonder richting) te pakken: alles voor de laatste underscore\n",
        "def base_sensor(col):\n",
        "    return col.rsplit('_',1)[0] if '_' in col else col\n",
        "\n",
        "base_map = {c: base_sensor(c) for c in sensor_cols}\n",
        "\n",
        "# 4.b Som over richtingen per basenaam\n",
        "df_summed = df[['timestamp']].copy()\n",
        "for base in sorted(set(base_map.values())):\n",
        "    cols = [c for c,b in base_map.items() if b == base]\n",
        "    df_summed[base] = df[cols].sum(axis=1)\n",
        "\n",
        "# 4.c Long-form\n",
        "long_df = df_summed.melt(id_vars='timestamp', var_name='zone_id', value_name='flow_3min')\n",
        "long_df['density'] = long_df['flow_3min'].astype(float)  # proxy\n",
        "long_df = long_df[['timestamp','zone_id','density']].sort_values(['zone_id','timestamp']).reset_index(drop=True)\n",
        "\n",
        "display(long_df.head())\n",
        "print(\"Rows:\", len(long_df), \"Zones:\", long_df['zone_id'].nunique(),\n",
        "      \"Start:\", long_df['timestamp'].min(), \"End:\", long_df['timestamp'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sensorlocaties → GeoJSON\n",
        "\n",
        "- We lezen `Objectummer` (sensor-id), `Locatienaam`, `Lat/Long`.\n",
        "- `Lat/Long` wordt geparsed (komma's → punten).\n",
        "- We maken per sensor een **klein vierkant** polygon (snel voor een choropleth).\n",
        "\n",
        "> Later kun je dit vervangen door echte polygonen of clusters per gebied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "loc = pd.read_csv(LOC_CSV)\n",
        "loc = loc.rename(columns={'Objectummer':'zone_id', 'Locatienaam':'name', 'Lat/Long':'latlon'})\n",
        "\n",
        "def parse_latlon(s):\n",
        "    s = str(s).strip()\n",
        "    # Soms 'lat, lon' met komma's als scheiding en decimaal—we filteren getallen eruit\n",
        "    parts = [p.strip() for p in s.replace(';', ',').split(',')]\n",
        "    nums = []\n",
        "    for p in parts:\n",
        "        try:\n",
        "            nums.append(float(p.replace(' ','').replace(',', '.')))\n",
        "        except:\n",
        "            pass\n",
        "    if len(nums) >= 2:\n",
        "        return nums[0], nums[1]\n",
        "    return np.nan, np.nan\n",
        "\n",
        "lat, lon = zip(*[parse_latlon(v) for v in loc['latlon']])\n",
        "loc['lat'] = lat\n",
        "loc['lon'] = lon\n",
        "loc = loc[['zone_id','name','lat','lon']]\n",
        "\n",
        "# Hou alleen sensoren die ook in de flows zitten\n",
        "present_ids = set(long_df['zone_id'].unique()) & set(loc['zone_id'].unique())\n",
        "loc = loc[loc['zone_id'].isin(present_ids)].copy()\n",
        "long_df = long_df[long_df['zone_id'].isin(present_ids)].copy()\n",
        "\n",
        "def square_around(lat, lon, dlat=0.0008, dlon=0.0012):\n",
        "    return [\n",
        "        [lon-dlon, lat-dlat],\n",
        "        [lon+dlon, lat-dlat],\n",
        "        [lon+dlon, lat+dlat],\n",
        "        [lon-dlon, lat+dlat],\n",
        "        [lon-dlon, lat-dlat]\n",
        "    ]\n",
        "\n",
        "features = []\n",
        "for _, r in loc.iterrows():\n",
        "    if np.isnan(r['lat']) or np.isnan(r['lon']):\n",
        "        continue\n",
        "    poly = square_around(r['lat'], r['lon'])\n",
        "    features.append({\n",
        "        \"type\": \"Feature\",\n",
        "        \"properties\": {\"zone_id\": r['zone_id'], \"name\": r.get('name', r['zone_id'])},\n",
        "        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [poly]}\n",
        "    })\n",
        "\n",
        "gj = {\"type\":\"FeatureCollection\",\"features\":features}\n",
        "with open(OUT_GEOJSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(gj, f)\n",
        "\n",
        "print(f\"GeoJSON sensors: {len(features)} → {OUT_GEOJSON}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Exporteer timeseries voor dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Minimale structuur voor dashboard/notebooks\n",
        "timeseries = long_df[['timestamp','zone_id','density']].copy()\n",
        "\n",
        "# (Optioneel: in toekomst kun je echte features joinen)\n",
        "timeseries['pt_arrivals'] = np.nan\n",
        "timeseries['temp'] = np.nan\n",
        "timeseries['wind'] = np.nan\n",
        "timeseries['special_event'] = 0\n",
        "\n",
        "timeseries.to_csv(OUT_TIMESERIES, index=False)\n",
        "print(\"OK →\", OUT_TIMESERIES, \"rows:\", len(timeseries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Exporteer features-bestand (met placeholders)\n",
        "\n",
        "We maken alvast `crowd_timeseries_features.csv` zodat je later eenvoudig **AIS/KNMI/WBGT** kunt toevoegen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "features = timeseries.rename(columns={\n",
        "    'temp':'temp_C', 'wind':'wind_mps'  # align met latere naamgeving\n",
        "}).copy()\n",
        "\n",
        "# Voeg WBGT & humidity (rh_pct) als lege placeholder-kolommen toe\n",
        "features['rh_pct'] = np.nan\n",
        "features['wbgt_C'] = np.nan\n",
        "\n",
        "# Voeg AIS placeholders toe\n",
        "features['vessel_count'] = np.nan\n",
        "features['vessel_speed_ms'] = np.nan\n",
        "\n",
        "# Herordenen\n",
        "cols = ['timestamp','zone_id','density','vessel_count','vessel_speed_ms','temp_C','wind_mps','rh_pct','wbgt_C']\n",
        "features = features[cols].sort_values(['zone_id','timestamp']).reset_index(drop=True)\n",
        "\n",
        "features.to_csv(OUT_FEATURES, index=False)\n",
        "print(\"OK →\", OUT_FEATURES, \"rows:\", len(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Samenvatting & wat nu?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ✅ **Gemaakt:**  \n",
        "  - `/mnt/data/crowd_timeseries.csv`  \n",
        "  - `/mnt/data/zones.geojson`  \n",
        "  - `/mnt/data/crowd_timeseries_features.csv`  \n",
        "\n",
        "- 🔁 **Volgende stappen (optioneel):**  \n",
        "  - Voeg **AIS** toe → tel `vessel_count` binnen straal per zone & 3-min-bin; vul `vessel_speed_ms`.  \n",
        "  - Voeg **KNMI** toe → vul `temp_C`, `wind_mps`, `rh_pct`.  \n",
        "  - Voeg **WBGT** toe → vul `wbgt_C` vanuit KNMI WBGT feed.  \n",
        "\n",
        "- 🧭 **Gebruik in je dashboard/notebook:**  \n",
        "  - `data_path = \"/mnt/data/crowd_timeseries_features.csv\"`  \n",
        "  - `geo_path  = \"/mnt/data/zones.geojson\"`  \n",
        "  - `step_minutes = 3`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}